{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tage and Store\n",
    "This workflow will tag information.\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install python-dotenv langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# Datamodel representing a single tag with a description.\n",
    "class Tag(BaseModel):\n",
    "    tag: str = Field(description=\"The tag it self\")\n",
    "    description: str = Field(description=\"A brief description of the tag\")\n",
    "\n",
    "\n",
    "# Datamodel representing the structured tags for a document.\n",
    "class DocumentTags(BaseModel):\n",
    "    primary_topic: str = Field(description=\"The main topic or subject of the document\")\n",
    "    tags: List[Tag] = Field(description=\"List of relevant tags for the document\")\n",
    "    summary: str = Field(description=\"A brief summary of the document content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "def generate_document_tags(content: str, prompt_template: str, model) -> DocumentTags:\n",
    "    \"\"\"\n",
    "    Generates structured tags for a given document content.\n",
    "    \n",
    "    Args:\n",
    "        content: The document text content\n",
    "        model: The LLM model to use\n",
    "        rate_limiter: Optional rate limiter for API calls\n",
    "    \n",
    "    Returns:\n",
    "        DocumentTags object containing generated tags and metadata\n",
    "    \"\"\"\n",
    "    # Initialize the output parser\n",
    "    parser = PydanticOutputParser(pydantic_object=DocumentTags)\n",
    "    \n",
    "    # Assemble the prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"content\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "        \n",
    "    # Create and run the chain\n",
    "    chain = prompt | model | parser\n",
    "    result = chain.invoke({\"content\": content})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "with open(\"prompt.txt\", \"r\") as f:\n",
    "    template = f.read()\n",
    "\n",
    "with open(\"example_document.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize your model and rate limiter here\n",
    "model = ChatOpenAI(\n",
    "    model=os.getenv('gpt-4o-mini'),\n",
    "    temperature=0,\n",
    "    seed=42,\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the documents\n",
    "result = generate(data, template, model)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
