{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/microsoft/table-transformer-structure-recognition-v1.1-all\n",
    "# https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52fd0ae2015a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TableTransformerForObjectDetection, DetrImageProcessor\n",
    "from PIL import Image\n",
    "# import requests\n",
    "\n",
    "\n",
    "# Load the model and processor\n",
    "model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-structure-recognition-v1.1-all\")\n",
    "processor = DetrImageProcessor.from_pretrained(\"microsoft/table-transformer-structure-recognition-v1.1-all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affb6d4a6a1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"table.png\")\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979ec9d9551596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bounding boxes and labels\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "\n",
    "# Extract table structure and text content\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    if score > 0.5:  # Threshold for confidence\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54250602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder=\"output_images\"):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Convert PDF to a list of images\n",
    "    images = convert_from_path(pdf_path, dpi=200)  # dpi can be adjusted based on desired quality\n",
    "\n",
    "    # Save images to the output folder\n",
    "    image_files = []\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "        image.save(image_path, 'PNG')\n",
    "        image_files.append(image_path)\n",
    "    \n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241194da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "pdf_path = os.getenv(\"PDF_PATH\")\n",
    "image_files = convert_pdf_to_images(pdf_path)\n",
    "print(\"Images saved:\", image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification\n",
    "from PIL import Image\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Load processor and model\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "model = LayoutLMv2ForTokenClassification.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "# Load image\n",
    "image = Image.open(os.getenv(\"PDF_IMAGE_PATH\"))\n",
    "words = [\"Hello\", \"world\"]  # List of words recognized in the OCR process (you would use an actual OCR tool here)\n",
    "boxes = [[27, 76, 91, 112], [95, 73, 191, 113]]  # Example bounding boxes for each word\n",
    "\n",
    "# Prepare encoding\n",
    "encoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(**encoding)\n",
    "\n",
    "# Post-process here to extract desired output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
